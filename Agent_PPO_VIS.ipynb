{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datvodinh10/Truly-Proximal-Policy-Optimization/blob/main/Agent_PPO_VIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoM1jzlh3IT6"
      },
      "outputs": [],
      "source": [
        "game_name = 'Catan'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2bwk4Xc3IT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3080be5d-40f3-43eb-b6e6-f6f3c3cd0a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "PATH = \"./\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# PATH = f\"/content/gdrive/MyDrive/Data 12 Hour/{game_name}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdEj5YVF3IT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd149baa-f43f-495e-9304-b420bb46383a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ]
        }
      ],
      "source": [
        "!lscpu | grep 'Model name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z_hzbFk3IT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56fc271-83d4-456b-ed67-87b3b64192f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ENV'...\n",
            "remote: Enumerating objects: 4910, done.\u001b[K\n",
            "remote: Counting objects: 100% (962/962), done.\u001b[K\n",
            "remote: Compressing objects: 100% (500/500), done.\u001b[K\n",
            "remote: Total 4910 (delta 408), reused 935 (delta 393), pack-reused 3948\u001b[K\n",
            "Receiving objects: 100% (4910/4910), 276.01 MiB | 34.32 MiB/s, done.\n",
            "Resolving deltas: 100% (2070/2070), done.\n",
            "Updating files: 100% (1202/1202), done.\n",
            "/content/ENV\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ngoxuanphong/ENV.git\n",
        "%cd ENV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDYHpM1V3IT8"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os0at_jD3IT9"
      },
      "outputs": [],
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning,NumbaExperimentalFeatureWarning, NumbaWarning\n",
        "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
        "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
        "warnings.simplefilter('ignore', category=NumbaExperimentalFeatureWarning)\n",
        "warnings.simplefilter('ignore', category=NumbaWarning)\n",
        "\n",
        "from numba import njit\n",
        "from numba.typed import List\n",
        "import numba\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "from torch.distributions.kl import kl_divergence\n",
        "import numba\n",
        "from numba import njit,jit\n",
        "from numba.typed import List\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOvBWB__3IT9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB8_Y4VS3IT-"
      },
      "outputs": [],
      "source": [
        "from setup import make\n",
        "env = make(game_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY1fVS7i3IT-"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2hDKH7t3IT_"
      },
      "outputs": [],
      "source": [
        "KPI = 1 / env.getAgentSize() + 0.01 * env.getAgentSize()\n",
        "TIME = 3600 * 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZXWK2CA3IT_"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38nDnBZl3IT_"
      },
      "outputs": [],
      "source": [
        "def StandardScaler(X):\n",
        "    \"\"\"Return Data with Standard Scaling\"\"\"\n",
        "    data     = X.T\n",
        "    new_mean = torch.zeros(X.shape[1])\n",
        "    new_std  = torch.zeros(X.shape[1])\n",
        "    for i in range(data.shape[0]):\n",
        "        new_mean[i] = torch.mean(data[i])\n",
        "        new_std[i]  = torch.std(data[i])\n",
        "\n",
        "    new_mean = new_mean.reshape(1,-1)\n",
        "    new_std  = new_std.reshape(1,-1)\n",
        "    return (X - new_mean) / (new_std + 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dhb3olEM3IT_"
      },
      "outputs": [],
      "source": [
        "def LayerInit(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    \"\"\"Init Weight and Bias with Constraint\"\"\"\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJDnIg_k3IUA"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def MonteCarloRewards(rewards,is_terminals,rank,gamma):\n",
        "    \"\"\"Reward-to-go\"\"\"\n",
        "    rtgs = List.empty_list(numba.float32)\n",
        "    discounted_reward = 0\n",
        "    # rewards = (rewards - rewards.mean()) / (rewards.std()+1e-10)\n",
        "    for reward, is_terminal in np.column_stack((rewards[::-1],is_terminals[::-1])):\n",
        "        if is_terminal==1:\n",
        "            discounted_reward = 0\n",
        "        discounted_reward = reward + (gamma * discounted_reward)\n",
        "        rtgs.append(discounted_reward)\n",
        "    return rtgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OqmTTnn3IUA"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def RandomChoiceWithProb(arr, prob):\n",
        "    \"\"\"Choice with given Probability\"\"\"\n",
        "    return arr[np.searchsorted(np.cumsum(prob), np.random.rand(), side=\"right\")]\n",
        "@njit\n",
        "def StableSoftmax(x):\n",
        "    \"\"\"Return Softmax of the output\"\"\"\n",
        "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S74k-Cqd3IUA"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def ChooseAction(state,per,prob=True):\n",
        "    \"\"\"Return Action with given State and Per\"\"\"\n",
        "    param0,param1,param2,param3,param4,param5 = per[-6][0],per[-5][0],per[-4][0],per[-3][0],per[-2][0],per[-1][0]\n",
        "    list_action = np.where(env.getValidActions(state)==1)[0]\n",
        "    out1        = np.dot(state.reshape(1,-1).astype(np.float32),param0) + param1\n",
        "    out2        = np.dot(np.tanh(out1),param2) + param3\n",
        "    policy      = np.dot(np.tanh(out2),param4) + param5\n",
        "    if prob:\n",
        "        return RandomChoiceWithProb(list_action,StableSoftmax(policy[0][list_action]))\n",
        "    else:\n",
        "        return list_action[np.argmax(policy[0][list_action])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K-h4_lP3IUA"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def Play(state,per):\n",
        "    \"\"\"Agent to get Data\"\"\"\n",
        "    action = ChooseAction(state,per)\n",
        "    valid_action =  env.getValidActions(state)\n",
        "    if valid_action[action] != 1: # Prevent Underflow cause invalid action\n",
        "        print(action,valid_action)\n",
        "        action = np.random.choice(np.where(valid_action==1)[0])\n",
        "    if env.getReward(state)==-1:\n",
        "        per[0].append(np.array([[action]],dtype=np.float32))#action\n",
        "        per[1].append(state.reshape(1,-1).astype(np.float32))#state\n",
        "        per[2].append(np.array([[-0.001]],dtype=np.float32))#reward\n",
        "        per[3].append(np.array([[0.]],dtype=np.float32))#is_terminals\n",
        "        per[4].append(env.getValidActions(state).reshape(1,-1).astype(np.float32))#action masking\n",
        "    else:\n",
        "        per[0].append(np.array([[action]],dtype=np.float32))\n",
        "        per[1].append(state.reshape(1,-1).astype(np.float32))\n",
        "        per[2].append(np.array([[env.getReward(state)*1.0]],dtype=np.float32))\n",
        "        per[3].append(np.array([[1.]],dtype=np.float32))\n",
        "        per[4].append(env.getValidActions(state).reshape(1,-1).astype(np.float32))\n",
        "    return action,per"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A728Z_CE3IUB"
      },
      "outputs": [],
      "source": [
        "class ActorModel(nn.Module):\n",
        "    \"\"\"Actor Model\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            LayerInit(nn.Linear(env.getStateSize(),64)),\n",
        "            nn.Tanh(),\n",
        "            LayerInit(nn.Linear(64,64)),\n",
        "            nn.Tanh(),\n",
        "            LayerInit(nn.Linear(64,env.getActionSize()),std=0.01)\n",
        "        )\n",
        "\n",
        "class CriticModel(nn.Module):\n",
        "    \"\"\"Critic Model\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "                LayerInit(nn.Linear(env.getStateSize(),128)),\n",
        "                nn.Tanh(),\n",
        "                LayerInit(nn.Linear(128,128)),\n",
        "                nn.Tanh(),\n",
        "                LayerInit(nn.Linear(128,1),std=1)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p7vv3513IUB"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self,num_games = 10,num_game_per_batch=128,n_iter=5,lr=1e-3,batch_size=2048,lr_decay=1,entropy_coef=0,critic_coef = 1,gamma=0.995,value_clip = 0.2,policy_params=20,policy_kl_range=0.0008):\n",
        "        super().__init__()\n",
        "        self.actor                  = ActorModel().actor\n",
        "        self.critic                 = CriticModel().critic\n",
        "        \n",
        "        self.num_game_per_batch     = num_game_per_batch\n",
        "        self.n_iters                = n_iter\n",
        "        self.lr                     = lr\n",
        "        self.lr_decay               = lr_decay\n",
        "        self.batch_size             = batch_size\n",
        "        self.optimizer              = torch.optim.Adam([\n",
        "                                        {'params': self.actor.parameters(), 'lr': self.lr},\n",
        "                                        {'params': self.critic.parameters(), 'lr': self.lr}\n",
        "                                    ])\n",
        "        self.critic_coef            = critic_coef\n",
        "        self.entropy_coef           = entropy_coef\n",
        "        self.gamma                  = gamma\n",
        "        self.num_games              = num_games\n",
        "        self.value_clip             = value_clip\n",
        "        self.policy_kl_range        = policy_kl_range\n",
        "        self.policy_params          = policy_params\n",
        "        self.best_actor_state_dict  = self.actor.state_dict()\n",
        "        self.best_critic_state_dict = self.critic.state_dict()\n",
        "\n",
        "        self.entropy_data           = []\n",
        "        self.mean_win_data          = []\n",
        "        self.time_data              = []\n",
        "    # @torch.jit.script_method\n",
        "    def Forward(self,state):\n",
        "        return self.actor(state),self.critic(state)\n",
        "    def GetPolicy(self,state):\n",
        "        return self.actor(state)\n",
        "    # @torch.jit.script_method\n",
        "    \n",
        "    def CalculateTrulyLoss(self,value,value_new,entropy,log_prob,log_prob_new,rtgs):\n",
        "        \"\"\"Calculate Model Loss\"\"\"\n",
        "        advantage       = rtgs - value.detach()\n",
        "        ratios          = torch.exp(torch.clamp(log_prob_new-log_prob.detach(),min=-20.,max=5.))\n",
        "        Kl              = kl_divergence(Categorical(logits=log_prob), Categorical(logits=log_prob_new))\n",
        "\n",
        "        actor_loss      = -torch.where(\n",
        "                            (Kl >= self.policy_kl_range) & (ratios >= 1),\n",
        "                            ratios * advantage - self.policy_params * Kl,\n",
        "                            ratios * advantage\n",
        "                        ).mean()\n",
        "        # print(actor_loss)\n",
        "        value_clipped   = value + torch.clamp(value_new - value, -self.value_clip, self.value_clip)\n",
        "\n",
        "        critic_loss     = 0.5 * torch.max((rtgs-value_new)**2,(rtgs-value_clipped)**2).mean()\n",
        "        total_loss      = actor_loss + self.critic_coef * critic_loss - self.entropy_coef * entropy\n",
        "        # print(ratios.shape,actor_loss,critic_loss,entropy,total_loss)\n",
        "        # print(advantage,ratios,Kl,actor_loss,critic_loss,total_loss)\n",
        "        return total_loss\n",
        "    \n",
        "    def UpdatePer(self,actor_state_dict):\n",
        "        \"\"\"Update per file\"\"\"\n",
        "        perx = [List.empty_list(numba.types.Array(dtype=numba.float32,ndim=2,layout='C')),\n",
        "                List.empty_list(numba.types.Array(dtype=numba.float32,ndim=2,layout='C')),\n",
        "                List.empty_list(numba.types.Array(dtype=numba.float32,ndim=2,layout='C')),\n",
        "                List.empty_list(numba.types.Array(dtype=numba.float32,ndim=2,layout='C')),\n",
        "                List.empty_list(numba.types.Array(dtype=numba.float32,ndim=2,layout='C')),\n",
        "                List.empty_list(numba.types.Array(dtype=numba.float32,ndim=2,layout='C')),\n",
        "                List.empty_list(numba.types.Array(dtype=numba.float32,ndim=2,layout='C'))]\n",
        "\n",
        "        param0 = actor_state_dict['0.weight'].detach().numpy().T\n",
        "        param1 = actor_state_dict['0.bias'].detach().numpy().reshape(1,-1)\n",
        "        param2 = actor_state_dict['2.weight'].detach().numpy().T\n",
        "        param3 = actor_state_dict['2.bias'].detach().numpy().reshape(1,-1)\n",
        "        param4 = actor_state_dict['4.weight'].detach().numpy().T\n",
        "        param5 = actor_state_dict['4.bias'].detach().numpy().reshape(1,-1)\n",
        "        params = [param0,param1,param2,param3,param4,param5]\n",
        "        for param in params:\n",
        "            perx.append(List([np.array(param,ndmin=2,order='C')]))\n",
        "        return perx\n",
        "    \n",
        "    def plot(self,plot_graph=True):\n",
        "        \"\"\"Plot data\"\"\"\n",
        "        sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
        "        plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
        "        plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
        "        plt.rc('xtick', labelsize=13)    # fontsize of the tick labels\n",
        "        plt.rc('ytick', labelsize=13)    # fontsize of the tick labels\n",
        "        plt.rc('legend', fontsize=13)    # legend fontsize\n",
        "        plt.rc('font', size=13)\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(self.time_data,self.entropy_data)\n",
        "        plt.title('Entropy')\n",
        "        plt.xlabel('Time(s)')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(self.time_data,self.mean_win_data)\n",
        "        plt.title('Avg Win Rate(%)')\n",
        "        plt.xlabel('Time(s)')\n",
        "        if plot_graph:\n",
        "            plt.show()\n",
        "        plt.savefig(f'{PATH}{game_name}-Plot.png')\n",
        "        \n",
        "    def TrainModel(self,num_games=100,lr_decay=False,entropy_decay=True,level=1,save_every_epochs = 100,print_every_epochs=100,time_kpi = 3600):\n",
        "        \"\"\"Training model\"\"\"\n",
        "        global perx\n",
        "        self.num_games  = num_games // self.num_game_per_batch\n",
        "        list_rank       = []\n",
        "        NUM_MEAN        = 10_000 / self.num_game_per_batch\n",
        "        best_win_rate   = -100\n",
        "\n",
        "        start = time.time()\n",
        "        \n",
        "        pass_kpi = False\n",
        "        for i in range(self.num_games):\n",
        "            perx = self.UpdatePer(self.actor.state_dict())\n",
        "            s1 = time.time()\n",
        "            rank = env.numba_main_2(Play,self.num_game_per_batch,perx,level)[0] / self.num_game_per_batch\n",
        "            # rank = numba_main(Play,RandomPlayer,RandomPlayer,RandomPlayer,self.num_game_per_batch,perx)[0][0] / self.num_game_per_batch\n",
        "            \n",
        "            if i%print_every_epochs==0:\n",
        "                print(f\"| BATCH: {i:>5} | RUN: {str(f'{time.time()-s1:.2f}')+'s':>7} | WIN RATE: {str(f'{rank*100:.2f}')+'%':>7} |\",end=\" \" )\n",
        "            \n",
        "            batch_actions           = torch.as_tensor(np.array(perx[0]).squeeze(),dtype=torch.float32).reshape(-1).detach()\n",
        "            batch_states            = torch.as_tensor(np.array(perx[1]).squeeze(),dtype=torch.float32).detach()\n",
        "            batch_rtgs              = np.flip(MonteCarloRewards(np.array(perx[2]).squeeze(),np.array(perx[3]).squeeze(),rank,self.gamma),axis=0)\n",
        "            batch_rtgs              = torch.as_tensor(batch_rtgs.copy(),dtype=torch.float32).detach()\n",
        "            batch_mask              = torch.as_tensor(np.array(perx[4]).squeeze(),dtype=torch.float32).detach()\n",
        "            # if torch.min(batch_mask)<0:\n",
        "            #     batch_mask              = (batch_mask >= 0).astype(torch.float32)\n",
        "            policy_old,batch_values = self.Forward(batch_states)\n",
        "            policy_old              = policy_old.detach()\n",
        "            batch_values            = batch_values.detach()\n",
        "            prob_old                = Categorical(logits=policy_old+torch.log(batch_mask))      \n",
        "            batch_logprobs          = prob_old.log_prob(batch_actions.view(1,-1)).detach().squeeze(0)\n",
        "            old_entropy             = prob_old.entropy().detach().mean().item()\n",
        "            for ix in range(6):\n",
        "                perx[ix].clear()\n",
        "            # if i%50==0:\n",
        "            #   print(batch_actions.shape,batch_states.shape,batch_rtgs.shape,batch_mask.shape,batch_values.shape)\n",
        "            if i%print_every_epochs==0:\n",
        "                print(f'ENTROPY: {old_entropy:.4f} |',end=\" \")\n",
        "            s2 = time.time()\n",
        "            for _ in range(self.n_iters):\n",
        "                n_samples = batch_states.shape[0]\n",
        "                # if i%print_every_epochs==0:\n",
        "                #   print(n_samples)\n",
        "                index = torch.randperm(n_samples)\n",
        "                states,actions,probs,rtgss,masks,values= batch_states[index],batch_actions[index],batch_logprobs[index],batch_rtgs[index],batch_mask[index],batch_values[index]\n",
        "                for idx in range(0, n_samples, self.batch_size):\n",
        "                    begin, end = idx, min(idx + self.batch_size, n_samples)\n",
        "                    if idx + self.batch_size > n_samples + 256:\n",
        "                        continue\n",
        "                    else:\n",
        "                        state,action,log_prob,rtgs,mask,value =  states[begin:end],actions[begin:end],probs[begin:end],rtgss[begin:end],masks[begin:end],values[begin:end]\n",
        "\n",
        "                        policy,value_new    = self.Forward(state)\n",
        "                        value_new           = value_new.squeeze(1)\n",
        "                        value               = value.squeeze(1)\n",
        "                        prob1               = Categorical(logits=policy+ torch.log(mask))\n",
        "                        log_prob_new        = prob1.log_prob(action.view(1,-1)).squeeze(0)\n",
        "                        \n",
        "                        entropy             = prob1.entropy().mean()\n",
        "                        total_loss          = self.CalculateTrulyLoss(value,value_new,entropy,log_prob,log_prob_new,rtgs)\n",
        "\n",
        "                        if not torch.isnan(total_loss).any():\n",
        "                            self.optimizer.zero_grad()\n",
        "                            total_loss.backward()\n",
        "                            nn.utils.clip_grad_norm_(self.parameters(),0.5)\n",
        "                            self.optimizer.step()\n",
        "                    \n",
        "                del actions\n",
        "                del states\n",
        "                del rtgss\n",
        "                del mask\n",
        "                del probs\n",
        "                del values \n",
        "\n",
        "            del batch_actions\n",
        "            del batch_states\n",
        "            del batch_rtgs\n",
        "            del batch_mask\n",
        "            del batch_logprobs\n",
        "            del batch_values\n",
        "            del policy_old\n",
        "            del prob_old\n",
        "            \n",
        "            perx = self.UpdatePer(self.actor.state_dict())\n",
        "\n",
        "            list_rank.append(rank)\n",
        "            if len(list_rank)>NUM_MEAN:\n",
        "                list_rank.pop(0)\n",
        "            win_rate_new = sum(list_rank) / NUM_MEAN\n",
        "\n",
        "            if i % print_every_epochs==0:\n",
        "                print(f\"TRAIN: {time.time()-s2:.2f}s | MEAN WIN RATE: {sum(list_rank) / NUM_MEAN * 100:.1f} % |\")\n",
        "                lst_per = []\n",
        "                for param in perx[-6:]:\n",
        "                    lst_per.append(param[0])\n",
        "\n",
        "            if i% save_every_epochs==0:\n",
        "                np.save(f'{PATH}per_{game_name}.npy',np.array(lst_per))\n",
        "                torch.save(self.best_actor_state_dict,f'{PATH}actor_state_dict_{game_name}.pt')\n",
        "                torch.save(self.best_critic_state_dict,f'{PATH}critic_state_dict_{game_name}.pt')\n",
        "                self.plot(plot_graph=False)\n",
        "\n",
        "            # /content/gdrive/MyDrive/Data Truly PPO 8 Hour/{game_name}\n",
        "\n",
        "            if lr_decay:\n",
        "                if i==2000:\n",
        "                    self.optimizer.param_groups[0]['lr'] = self.optimizer.param_groups[0]['lr'] * 0.2\n",
        "                    print(f\"LEARNING RATE CHANGE TO: {self.optimizer.param_groups[0]['lr']}\")\n",
        "            if entropy_decay:\n",
        "                if i==1000:\n",
        "                    self.entropy_coef = self.entropy_coef * 0.1\n",
        "                    print(f\"ENTROPY COEFFICIENT CHANGE TO: {self.entropy_coef}\")\n",
        "\n",
        "\n",
        "            #save data plot\n",
        "            if i%5==0:\n",
        "                self.entropy_data.append(old_entropy)\n",
        "                self.mean_win_data.append(win_rate_new*100)\n",
        "                self.time_data.append(time.time()-start)\n",
        "\n",
        "            if win_rate_new >= best_win_rate:\n",
        "                self.best_critic_state_dict = self.critic.state_dict()\n",
        "                self.best_actor_state_dict  = self.actor.state_dict()\n",
        "                best_win_rate               = win_rate_new\n",
        "\n",
        "            if best_win_rate - win_rate_new > 0.05: # Model start to underfit\n",
        "                self.actor.load_state_dict(self.best_actor_state_dict)\n",
        "                self.critic.load_state_dict(self.best_critic_state_dict)\n",
        "\n",
        "            if best_win_rate >= KPI and pass_kpi==False:\n",
        "                print(f\"TIME: {time.time()-start} s | PASS KPI! \")\n",
        "                pass_kpi = True\n",
        "            if time.time() - start >= time_kpi:\n",
        "                print('TRAINING 12 HOUR COMPLETED!')\n",
        "                print(f'TOTAL TIME: {time.time() - start:.2f} s')\n",
        "                perx    = self.UpdatePer(self.best_actor_state_dict)\n",
        "                lst_per = []\n",
        "                for param in perx[-6:]:\n",
        "                    lst_per.append(param[0])\n",
        "                np.save(f'{PATH}per_{game_name}.npy',np.array(lst_per))\n",
        "                torch.save(self.best_actor_state_dict,f'{PATH}actor_state_dict_{game_name}.pt')\n",
        "                torch.save(self.best_critic_state_dict,f'{PATH}critic_state_dict_{game_name}.pt')\n",
        "                self.plot(plot_graph=True)\n",
        "                # /content/gdrive/MyDrive/Data Truly PPO 8 Hour/{game_name}\n",
        "                break\n",
        "        return perx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCioCEFZ3IUC"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6V7BC7S3IUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5c2782-c066-499f-c2b3-64041c7ee279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| BATCH:     0 | RUN: 109.65s | WIN RATE:   0.00% | ENTROPY: 1.1997 | TRAIN: 1.50s | MEAN WIN RATE: 0.0 % |\n",
            "| BATCH:   100 | RUN:  13.07s | WIN RATE:  29.50% | ENTROPY: 1.0445 | TRAIN: 0.90s | MEAN WIN RATE: 23.8 % |\n",
            "TIME: 2108.5097312927246 s | PASS KPI! \n",
            "| BATCH:   200 | RUN:  13.51s | WIN RATE:  34.00% | ENTROPY: 0.9980 | TRAIN: 1.03s | MEAN WIN RATE: 34.7 % |\n",
            "| BATCH:   300 | RUN:  13.60s | WIN RATE:  44.00% | ENTROPY: 0.9481 | TRAIN: 1.16s | MEAN WIN RATE: 39.9 % |\n",
            "| BATCH:   400 | RUN:  13.78s | WIN RATE:  46.00% | ENTROPY: 0.8925 | TRAIN: 1.25s | MEAN WIN RATE: 48.0 % |\n",
            "| BATCH:   500 | RUN:  14.20s | WIN RATE:  61.00% | ENTROPY: 0.8468 | TRAIN: 1.57s | MEAN WIN RATE: 58.5 % |\n",
            "| BATCH:   600 | RUN:  13.55s | WIN RATE:  73.50% | ENTROPY: 0.7484 | TRAIN: 1.48s | MEAN WIN RATE: 68.6 % |\n",
            "| BATCH:   700 | RUN:  13.66s | WIN RATE:  75.00% | ENTROPY: 0.6972 | TRAIN: 1.53s | MEAN WIN RATE: 69.5 % |\n",
            "| BATCH:   800 | RUN:  13.66s | WIN RATE:  72.00% | ENTROPY: 0.6602 | TRAIN: 1.56s | MEAN WIN RATE: 71.3 % |\n",
            "| BATCH:   900 | RUN:  12.71s | WIN RATE:  66.00% | ENTROPY: 0.5881 | TRAIN: 1.46s | MEAN WIN RATE: 67.7 % |\n",
            "| BATCH:  1000 | RUN:  13.34s | WIN RATE:  70.00% | ENTROPY: 0.5818 | TRAIN: 1.54s | MEAN WIN RATE: 71.2 % |\n",
            "ENTROPY COEFFICIENT CHANGE TO: 0.0001\n",
            "| BATCH:  1100 | RUN:  13.42s | WIN RATE:  78.00% | ENTROPY: 0.5622 | TRAIN: 1.54s | MEAN WIN RATE: 73.9 % |\n",
            "| BATCH:  1200 | RUN:  12.50s | WIN RATE:  73.00% | ENTROPY: 0.5112 | TRAIN: 1.55s | MEAN WIN RATE: 71.1 % |\n",
            "| BATCH:  1300 | RUN:  13.29s | WIN RATE:  75.00% | ENTROPY: 0.5406 | TRAIN: 1.64s | MEAN WIN RATE: 71.9 % |\n",
            "| BATCH:  1400 | RUN:  12.76s | WIN RATE:  71.50% | ENTROPY: 0.5269 | TRAIN: 1.73s | MEAN WIN RATE: 73.5 % |\n",
            "| BATCH:  1500 | RUN:  13.09s | WIN RATE:  70.50% | ENTROPY: 0.4730 | TRAIN: 1.62s | MEAN WIN RATE: 72.0 % |\n",
            "| BATCH:  1600 | RUN:  12.57s | WIN RATE:  74.50% | ENTROPY: 0.4376 | TRAIN: 1.67s | MEAN WIN RATE: 74.5 % |\n",
            "| BATCH:  1700 | RUN:  12.31s | WIN RATE:  72.00% | ENTROPY: 0.4405 | TRAIN: 1.81s | MEAN WIN RATE: 73.7 % |\n",
            "| BATCH:  1800 | RUN:  13.36s | WIN RATE:  73.50% | ENTROPY: 0.3416 | TRAIN: 1.67s | MEAN WIN RATE: 73.0 % |\n"
          ]
        }
      ],
      "source": [
        "if env.getStateSize() > 450: #prevent out of memory\n",
        "    agent  = Agent(num_games=1,num_game_per_batch=50,n_iter=2,lr=1e-3,batch_size=512,gamma=1,entropy_coef=0.01,value_clip=0.2)\n",
        "    perx = agent.TrainModel(num_games=100_000_000,level=1,time_kpi = TIME,save_every_epochs=25,lr_decay=False)\n",
        "elif env.getActionSize() < 16:\n",
        "    agent  = Agent(num_games=1,num_game_per_batch=200,n_iter=2,lr=1e-3,batch_size=1024,gamma=1,entropy_coef=0,value_clip=0.2)\n",
        "    perx = agent.TrainModel(num_games=100_000_000,level=1,time_kpi = TIME,save_every_epochs=200,lr_decay=False)\n",
        "else:\n",
        "    agent  = Agent(num_games=1,num_game_per_batch=200,n_iter=2,lr=1e-3,batch_size=1024,gamma=1,entropy_coef=0.001,value_clip=0.2)\n",
        "    perx = agent.TrainModel(num_games=100_000_000,level=1,time_kpi = TIME,save_every_epochs=100,lr_decay=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQn2MA8b3IUD"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCrYj7_3IUD"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def Test_per(state,per):\n",
        "    param0,param1,param2,param3,param4,param5 = per[-6],per[-5],per[-4],per[-3],per[-2],per[-1]\n",
        "    list_action = np.where(env.getValidActions(state)==1)[0]\n",
        "    out1 = np.dot(state.reshape(1,-1).astype(np.float32),param0) + param1\n",
        "    out2 = np.dot(np.tanh(out1),param2) + param3\n",
        "    policy = np.dot(np.tanh(out2),param4) + param5\n",
        "    return RandomChoiceWithProb(list_action,StableSoftmax(policy[0][list_action])),per\n",
        "\n",
        "per_file = list(np.load(f'{PATH}per_{game_name}.npy',allow_pickle=True))\n",
        "num_test = 10_000\n",
        "win = env.numba_main_2(Test_per,num_test,per_file,1)[0]\n",
        "print(f'| GAME: {game_name:<18} | WIN RATE: {win / num_test * 100:.2f} %{\"\":>2} vs KPI: {KPI*100:.1f} % |')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}